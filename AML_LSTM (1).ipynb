{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb293d4-e146-47b7-9261-cc36faad0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  \n",
    "import time \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad5461-7e06-415e-8d64-b250798b79c7",
   "metadata": {},
   "source": [
    "This file uses LSTM model to predict the following week's feature(just danceability). The training process is using the 2022's data to train and the data of 2023 to validate(Same dataset as Holt-Winters), The validation MSE of danceability is 4.5530e-05, which is not as good as Holt-Winters(2e-05)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f7ce72-2e0a-4ef7-8bf5-b45e6c9a0a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndaily_mean = {'Date': Date, 'Mean_dance': Mean_dance,'Mean_energy':Mean_energy,'Mean_loud':Mean_loud,'Mean_speech':Mean_speech,'Mean_acoustic':Mean_acoustic,'Mean_instru':Mean_instru,'Mean_valence':Mean_valence}\\ndailymean_df = pd.DataFrame(data=daily_mean)\\ndaily_var = {'Date': Date, 'Var_dance': Var_dance,'Var_energy':Var_energy,'Var_loud':Var_loud,'Var_speech':Var_speech,'Var_acoustic':Var_acoustic,'Var_instru':Var_instru,'Var_valence':Var_valence}\\ndailyvar_df = pd.DataFrame(data=daily_var)\\nDaily_mean = {'Mean_dance': Mean_dance,'Mean_energy':Mean_energy,'Mean_speech':Mean_speech,'Mean_acoustic':Mean_acoustic,'Mean_instru':Mean_instru,'Mean_valence':Mean_valence}\\nDailydata = pd.DataFrame(data=Daily_mean)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\Windows\\Desktop\\Spotify_Dataset_V3.csv',delimiter=';')\n",
    "df_noURL=df.iloc[:,0:-1]\n",
    "url=df.iloc[:,-1]\n",
    "# Creat a daily average data of each feature\n",
    "df_nodup = df_noURL.loc[:,['Title','Artists','Date','Danceability','Energy','Loudness','Speechiness','Acousticness','Instrumentalness','Valence']]\n",
    "df_nodup = df_nodup.drop_duplicates(subset=['Title','Artists','Date'])\n",
    "df_nodup = df_nodup.drop('Artists', axis=1)\n",
    "df_nodup = df_nodup.iloc[::-1]\n",
    "Date = [];Mean_dance=[];Mean_energy=[];Mean_loud=[];Mean_speech=[];Mean_acoustic=[];Mean_instru=[];Mean_valence=[]\n",
    "Var_dance=[];Var_energy=[];Var_loud=[];Var_speech=[];Var_acoustic=[];Var_instru=[];Var_valence=[]\n",
    "i=199;k=0\n",
    "while (df_nodup.shape[0]-i)>0:\n",
    "    date=datetime.strptime(df_nodup.iloc[i,1],'%d/%m/%Y').strftime('%Y-%m-%d')\n",
    "    if i>200:\n",
    "        if date == Date[-1]:\n",
    "            k=k+1\n",
    "            i=i+1\n",
    "            continue\n",
    "    Date.append(date)\n",
    "    df_mean=df_nodup.iloc[i-199-k:i,2:].mean()\n",
    "    df_var=df_nodup.iloc[i-199-k:i,2:].var()\n",
    "    Mean_dance.append(df_mean.iloc[0]);Var_dance.append(df_var.iloc[0])\n",
    "    Mean_energy.append(df_mean.iloc[1]);Var_energy.append(df_var.iloc[1])\n",
    "    Mean_loud.append(df_mean.iloc[2]);Var_loud.append(df_var.iloc[2])\n",
    "    Mean_speech.append(df_mean.iloc[3]);Var_speech.append(df_var.iloc[3])\n",
    "    Mean_acoustic.append(df_mean.iloc[4]);Var_acoustic.append(df_var.iloc[4])\n",
    "    Mean_instru.append(df_mean.iloc[5]);Var_instru.append(df_var.iloc[5])\n",
    "    Mean_valence.append(df_mean.iloc[6]);Var_valence.append(df_var.iloc[6])\n",
    "    i+=200\n",
    "    k=0\n",
    "'''\n",
    "daily_mean = {'Date': Date, 'Mean_dance': Mean_dance,'Mean_energy':Mean_energy,'Mean_loud':Mean_loud,'Mean_speech':Mean_speech,'Mean_acoustic':Mean_acoustic,'Mean_instru':Mean_instru,'Mean_valence':Mean_valence}\n",
    "dailymean_df = pd.DataFrame(data=daily_mean)\n",
    "daily_var = {'Date': Date, 'Var_dance': Var_dance,'Var_energy':Var_energy,'Var_loud':Var_loud,'Var_speech':Var_speech,'Var_acoustic':Var_acoustic,'Var_instru':Var_instru,'Var_valence':Var_valence}\n",
    "dailyvar_df = pd.DataFrame(data=daily_var)\n",
    "Daily_mean = {'Mean_dance': Mean_dance,'Mean_energy':Mean_energy,'Mean_speech':Mean_speech,'Mean_acoustic':Mean_acoustic,'Mean_instru':Mean_instru,'Mean_valence':Mean_valence}\n",
    "Dailydata = pd.DataFrame(data=Daily_mean)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d42733-7f67-489f-bf0a-9a84ed9e346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        L.seed_everything(seed=41)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "         \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector\n",
    "        for input_i in range(input.size(0)):\n",
    "            input_trans = input[input_i].view(len(input[input_i]),1)\n",
    "            lstm_out, temp = self.lstm(input_trans)\n",
    "            ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "            if input_i==0:\n",
    "                prediction=lstm_out[-1]\n",
    "            else:\n",
    "                prediction=torch.cat((prediction,lstm_out[-1]),0)\n",
    "        #prediction = prediction\n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.001) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "  \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = ((output_i - label_i)**2).mean() ## loss = mean squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        '''\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "        '''\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058c0002-359e-4455-866a-81b6c8b2429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dancibility\n",
    "Train_data=[]\n",
    "for i in range(31):\n",
    "    train=Mean_dance[1823-31+i:2306-119-31+i]\n",
    "    Train_data.append(train)#2022.1.1-2022.11.30-2022.12.30\n",
    "Test_data=Mean_dance[2306-119-31:2306-119]#2022.12.1-2022.12.31\n",
    "#Train_test_data=Mean_dance[1822:2306-119]#2022.1.1-2022.12.31\n",
    "Validate_data_input=[]\n",
    "for j in range(7):\n",
    "    Validate_data_input.append(Mean_dance[2306-119+j:-7+j])#2023.1.1-2023.5.22\n",
    "Validate_data_output=Mean_dance[-7:]                          \n",
    "#Result_data=Mean_dance[2306-119:]#2023.1.1-2023.5.29 the data from model.result contains 7 days of pure prediction\n",
    "float32_array = np.array(Train_data).astype(np.float32)\n",
    "# Convert back to a list\n",
    "Train_data = float32_array.tolist()\n",
    "float32_array = np.array(Test_data).astype(np.float32)\n",
    "# Convert back to a list\n",
    "Test_data = float32_array.tolist()\n",
    "float32_array = np.array(Validate_data_input).astype(np.float32)\n",
    "# Convert back to a list\n",
    "Validate_data_input = float32_array.tolist()\n",
    "float32_array = np.array(Validate_data_output).astype(np.float32)\n",
    "# Convert back to a list\n",
    "Validate_data_output = float32_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce2ca823-e0f8-426e-affc-a69b3f2c16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.5271],\n",
      "        [-0.5468],\n",
      "        [ 0.6011],\n",
      "        [-0.6616]])\n",
      "lstm.weight_hh_l0 tensor([[-0.4701],\n",
      "        [ 0.5440],\n",
      "        [-0.7436],\n",
      "        [ 0.4904]])\n",
      "lstm.bias_ih_l0 tensor([0.6089, 0.2714, 0.1792, 0.3866])\n",
      "lstm.bias_hh_l0 tensor([ 0.7565,  0.0814, -0.7200,  0.9227])\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "tensor([-0.0818, -0.0821, -0.0823, -0.0825, -0.0823, -0.0789, -0.0778],\n",
      "       grad_fn=<CatBackward0>)\n",
      "[0.6613568067550659, 0.6609396934509277, 0.6602814197540283, 0.660804033279419, 0.6709195971488953, 0.6735929846763611, 0.6614372134208679]\n"
     ]
    }
   ],
   "source": [
    "model = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "    \n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(model(torch.tensor(Validate_data_input)))\n",
    "print(Validate_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c218b7a3-0092-4948-9242-019084a4a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor(Train_data)\n",
    "labels = torch.tensor(Test_data)\n",
    "dataset = TensorDataset(inputs.unsqueeze(0), labels.unsqueeze(0))\n",
    "#dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cae001b-ad39-4549-834a-0647206a38d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e817034a4f584630b170191d155eb907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.3083],\n",
      "        [-0.2816],\n",
      "        [ 0.8590],\n",
      "        [-0.4494]])\n",
      "lstm.weight_hh_l0 tensor([[-0.0613],\n",
      "        [ 0.9853],\n",
      "        [-0.4530],\n",
      "        [ 0.8947]])\n",
      "lstm.bias_ih_l0 tensor([0.8275, 0.5370, 0.4374, 0.5966])\n",
      "lstm.bias_hh_l0 tensor([ 0.9751,  0.3469, -0.4617,  1.1327])\n"
     ]
    }
   ],
   "source": [
    "## NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.\n",
    "## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    "## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,\n",
    "## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)\n",
    "## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So\n",
    "trainer = L.Trainer(max_epochs=1000)#log_every_n_steps=2\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b617b1-f5e0-4bba-8b94-653d45f44560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "let's compare the observed and predicted values...\n",
      "tensor([0.6613, 0.6605, 0.6598, 0.6592, 0.6588, 0.6606, 0.6629])\n",
      "[0.6613568067550659, 0.6609396934509277, 0.6602814197540283, 0.660804033279419, 0.6709195971488953, 0.6735929846763611, 0.6614372134208679]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nlet's compare the observed and predicted values...\")\n",
    "print(model(torch.tensor(Validate_data_input)).detach())\n",
    "print(Validate_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd81a23-2b5e-4640-8364-6f3f9b50a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE for a week\n",
      "tensor(4.5530e-05)\n"
     ]
    }
   ],
   "source": [
    "mse=((model(torch.tensor(Validate_data_input)).detach()-torch.tensor(Validate_data_output))**2).mean()\n",
    "print('Validation MSE for a week')\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb0946e-7ac8-42eb-a3da-8fabcf64172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_weekpredict(data, model):\n",
    "    \"\"\"\n",
    "        Returns prediction in the following week\n",
    "    \"\"\"\n",
    "    # errors array\n",
    "    predict=[]\n",
    "    for i in range(7):\n",
    "        pred = append(model(torch.tensor(data)).detach())\n",
    "        data = np.append(data,pred)\n",
    "        predict= predict.append(pred)\n",
    " \n",
    "    return np.array(predict)  # mean errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61444c-ffae-44f0-82dd-db3b82e3ce71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
